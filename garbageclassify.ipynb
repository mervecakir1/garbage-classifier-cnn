{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN8CuynfHILDhBQf4Qa7b1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mervecakir1/garbage-classifier-cnn/blob/main/garbageclassify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio plotly kaggle tqdm -q\n"
      ],
      "metadata": {
        "id": "ucPH0v1j8z_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\" GPU Aktif: {len(gpus)} GPU\")\n",
        "else:\n",
        "    print(\" GPU YOK! Runtime > Change runtime type > GPU seçin\")"
      ],
      "metadata": {
        "id": "f0f7dC0r9CSP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9SM83Yr9HHx",
        "outputId": "ca2dbe8d-2a1c-4686-dc7a-ddae49913c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/zlatan599/garbage-dataset-classification?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 121M/121M [00:06<00:00, 19.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/zlatan599/garbage-dataset-classification/versions/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/root/.cache/kagglehub/datasets/zlatan599/garbage-dataset-classification/versions/5/Garbage_Dataset_Classification/images\")\n",
        "\n",
        "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "def count_images(folder: Path, recursive: bool = False) -> int:\n",
        "    if recursive:\n",
        "        return sum(1 for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMAGE_EXTS)\n",
        "    else:\n",
        "        return sum(1 for p in folder.iterdir() if p.is_file() and p.suffix.lower() in IMAGE_EXTS)\n",
        "\n",
        "if not BASE.exists():\n",
        "    raise FileNotFoundError(f\"Klasör bulunamadı: {BASE}\")\n",
        "\n",
        "class_dirs = sorted([d for d in BASE.iterdir() if d.is_dir()])\n",
        "total = 0\n",
        "\n",
        "print(f\"Base klasör: {BASE}\\n\")\n",
        "for d in class_dirs:\n",
        "    n = count_images(d, recursive=False)\n",
        "    total += n\n",
        "    print(f\"{d.name:<12}: {n:5d}\")\n",
        "\n",
        "print(\"-\" * 24)\n",
        "print(f\"{'TOPLAM':<12}: {total:5d}\")\n"
      ],
      "metadata": {
        "id": "j0KZXnDTR4Xu",
        "outputId": "cae7b721-a0be-4e9d-cb2c-b6caf8a5c3ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base klasör: /root/.cache/kagglehub/datasets/zlatan599/garbage-dataset-classification/versions/5/Garbage_Dataset_Classification/images\n",
            "\n",
            "cardboard   :  2214\n",
            "glass       :  2500\n",
            "metal       :  2084\n",
            "paper       :  2315\n",
            "plastic     :  2288\n",
            "trash       :  2500\n",
            "------------------------\n",
            "TOPLAM      : 13901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"zlatan599/garbage-dataset-classification\")\n",
        "print(\" Dataset path:\", path)\n",
        "\n",
        "def find_images_folder(base_path):\n",
        "    \"\"\"Images klasörünü recursive olarak bul\"\"\"\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        class_folders = [d for d in dirs if os.path.isdir(os.path.join(root, d))]\n",
        "        if len(class_folders) >= 4:\n",
        "            for class_folder in class_folders[:2]:\n",
        "                class_path = os.path.join(root, class_folder)\n",
        "                images = [f for f in os.listdir(class_path)\n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "                if len(images) > 0:\n",
        "                    return root\n",
        "    return None\n",
        "\n",
        "DATA_PATH = find_images_folder(path)\n",
        "\n",
        "if DATA_PATH:\n",
        "    print(f\"Veri yolu bulundu: {DATA_PATH}\")\n",
        "\n",
        "    classes = [d for d in os.listdir(DATA_PATH)\n",
        "              if os.path.isdir(os.path.join(DATA_PATH, d))]\n",
        "    print(f\" Sınıflar: {classes}\")\n",
        "    print(f\" Sınıf sayısı: {len(classes)}\")\n",
        "\n",
        "    total_images = 0\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(DATA_PATH, class_name)\n",
        "        images = [f for f in os.listdir(class_path)\n",
        "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        print(f\"   {class_name}: {len(images)} görsel\")\n",
        "        total_images += len(images)\n",
        "\n",
        "    print(f\"Toplam görsel: {total_images}\")\n",
        "else:\n",
        "    print(\"Veri yolu bulunamadı! Manuel kontrol edin:\")\n",
        "    print(\"Mevcut dosyalar:\")\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        level = root.replace(path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:\n",
        "            print(f\"{subindent}{file}\")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB3, ResNet50V2, DenseNet121\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout,\n",
        "                                   BatchNormalization, Concatenate, Input, Average)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Config:\n",
        "    DATA_PATH = DATA_PATH\n",
        "    IMG_SIZE = (224, 224)\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 40\n",
        "    LEARNING_RATE = 1e-4\n",
        "    PATIENCE = 12\n",
        "    ENSEMBLE_MODEL = \"ensemble_garbage_classifier.h5\"\n",
        "    EFFICIENTNET_MODEL = \"efficientnet_garbage.h5\"\n",
        "    RESNET_MODEL = \"resnet_garbage.h5\"\n",
        "    DENSENET_MODEL = \"densenet_garbage.h5\"\n",
        "    TFLITE_MODEL = \"ensemble_garbage_lite.tflite\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    config.DATA_PATH,\n",
        "    target_size=config.IMG_SIZE,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    config.DATA_PATH,\n",
        "    target_size=config.IMG_SIZE,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "num_classes = train_data.num_classes\n",
        "class_names = list(train_data.class_indices.keys())\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_data.classes),\n",
        "    y=train_data.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print(\"Class weights:\")\n",
        "for i, (class_name, weight) in enumerate(zip(class_names, class_weights)):\n",
        "    print(f\"   {class_name}: {weight:.2f}\")\n",
        "\n",
        "def create_base_model(model_name, input_shape, num_classes):\n",
        "    if model_name == \"efficientnet\":\n",
        "        base = EfficientNetB3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "        unfreeze_layers = 50\n",
        "    elif model_name == \"resnet\":\n",
        "        base = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "        unfreeze_layers = 40\n",
        "    elif model_name == \"densenet\":\n",
        "        base = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "        unfreeze_layers = 30\n",
        "\n",
        "    for layer in base.layers:\n",
        "        layer.trainable = False\n",
        "    for layer in base.layers[-unfreeze_layers:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D(name=f'{model_name}_gap')(x)\n",
        "    x = BatchNormalization(name=f'{model_name}_bn1')(x)\n",
        "    x = Dense(512, activation='relu', name=f'{model_name}_dense1')(x)\n",
        "    x = Dropout(0.5, name=f'{model_name}_dropout1')(x)\n",
        "    x = BatchNormalization(name=f'{model_name}_bn2')(x)\n",
        "    x = Dense(256, activation='relu', name=f'{model_name}_dense2')(x)\n",
        "    x = Dropout(0.3, name=f'{model_name}_dropout2')(x)\n",
        "    output = Dense(num_classes, activation='softmax', name=f'{model_name}_output')(x)\n",
        "\n",
        "    model = Model(inputs=base.input, outputs=output, name=f'{model_name}_model')\n",
        "    return model\n",
        "\n",
        "def create_ensemble_model(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape, name='ensemble_input')\n",
        "\n",
        "    efficientnet = create_base_model(\"efficientnet\", input_shape, num_classes)\n",
        "    resnet = create_base_model(\"resnet\", input_shape, num_classes)\n",
        "    densenet = create_base_model(\"densenet\", input_shape, num_classes)\n",
        "\n",
        "    eff_output = efficientnet(input_layer)\n",
        "    resnet_output = resnet(input_layer)\n",
        "    dense_output = densenet(input_layer)\n",
        "\n",
        "    ensemble_output = Average(name='ensemble_average')([eff_output, resnet_output, dense_output])\n",
        "    ensemble_model = Model(inputs=input_layer, outputs=ensemble_output, name='EnsembleGarbageClassifier')\n",
        "    return ensemble_model, [efficientnet, resnet, densenet]\n",
        "\n",
        "ensemble_model, individual_models = create_ensemble_model((*config.IMG_SIZE, 3), num_classes)\n",
        "\n",
        "print(f\" Total Parameters: {ensemble_model.count_params():,}\")\n",
        "\n",
        "ensemble_model.summary()\n",
        "\n",
        "ensemble_model.compile(\n",
        "    optimizer=AdamW(learning_rate=config.LEARNING_RATE, weight_decay=0.01),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_acc')]\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=config.PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.3,\n",
        "        patience=8,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1,\n",
        "        cooldown=3\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        config.ENSEMBLE_MODEL,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    )\n",
        "]\n",
        "\n",
        "history = ensemble_model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=config.EPOCHS,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    axes[0, 0].plot(history.history['accuracy'], 'b-', label='Training', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], 'r-', label='Validation', linewidth=2)\n",
        "    axes[0, 0].set_title('Ensemble Model Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].plot(history.history['loss'], 'b-', label='Training', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
        "    axes[0, 1].set_title('Ensemble Model Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].plot(history.history['top3_acc'], 'g-', label='Training Top-3', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_top3_acc'], 'orange', label='Validation Top-3', linewidth=2)\n",
        "    axes[1, 0].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Top-3 Accuracy')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].axis('off')\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    best_val_top3 = max(history.history['val_top3_acc'])\n",
        "    min_val_loss = min(history.history['val_loss'])\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "     ENSEMBLE TRAINING SUMMARY\n",
        "     Architecture: 3-Model Ensemble\n",
        "       • EfficientNetB3\n",
        "       • ResNet50V2\n",
        "       • DenseNet121\n",
        "     Best Metrics:\n",
        "       • Val Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\n",
        "       • Val Top-3 Acc: {best_val_top3:.4f} ({best_val_top3*100:.2f}%)\n",
        "       • Val Loss: {min_val_loss:.4f}\n",
        "     Training Info:\n",
        "       • Total Epochs: {len(history.history['accuracy'])}\n",
        "       • Batch Size: {config.BATCH_SIZE}\n",
        "       • Learning Rate: {config.LEARNING_RATE}\n",
        "    \"\"\"\n",
        "\n",
        "    axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,\n",
        "                   fontsize=11, verticalalignment='top',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightblue\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n",
        "print(f\" En iyi model kaydedildi: {config.ENSEMBLE_MODEL}\")"
      ],
      "metadata": {
        "id": "-1X553Uv9Rxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model(\"ensemble_garbage_classifier.h5\")\n",
        "print(\".h5 model yüklendi\")"
      ],
      "metadata": {
        "id": "Y6OoXa0z9r_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"ensemble_garbage_classifier.keras\")\n",
        "print(\".keras formatında kaydedildi\")"
      ],
      "metadata": {
        "id": "OfC3xxULWsmc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "h5_size = os.path.getsize(\"ensemble_garbage_classifier.h5\") / (1024*1024)\n",
        "keras_size = os.path.getsize(\"ensemble_garbage_classifier.keras\") / (1024*1024)\n"
      ],
      "metadata": {
        "id": "HgbElQaPWs9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "ensemble_model = keras.models.load_model(config.ENSEMBLE_MODEL)\n",
        "print(f\" En iyi model yüklendi: {config.ENSEMBLE_MODEL}\")\n",
        "\n",
        "val_data.reset()\n",
        "print(f\" Validation samples: {val_data.samples}\")\n",
        "\n",
        "predictions = ensemble_model.predict(val_data, verbose=1)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_true = val_data.classes\n",
        "\n",
        "print(f\" Tahmin edilen sample sayısı: {len(y_pred)}\")\n",
        "print(f\" Gerçek sample sayısı: {len(y_true)}\")\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=class_names, yticklabels=class_names,\n",
        "           square=True, cbar_kws={'shrink': 0.8})\n",
        "\n",
        "plt.title('Confusion Matrix - Ensemble Model', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens',\n",
        "           xticklabels=class_names, yticklabels=class_names,\n",
        "           square=True, cbar_kws={'shrink': 0.8})\n",
        "plt.title('Normalized Confusion Matrix - Ensemble Model', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\" Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\" Macro F1-Score: {macro_f1:.4f}\")\n",
        "print(f\" Weighted F1-Score: {weighted_f1:.4f}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name:<12} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<8}\")\n",
        "\n",
        "best_idx = np.argmax(f1)\n",
        "worst_idx = np.argmin(f1)\n",
        "print(f\"\\n Best Performance: {class_names[best_idx]} (F1: {f1[best_idx]:.4f})\")\n",
        "print(f\" Worst Performance: {class_names[worst_idx]} (F1: {f1[worst_idx]:.4f})\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "confidence_scores = np.max(predictions, axis=1)\n",
        "print(f\"Average Confidence: {np.mean(confidence_scores):.4f}\")\n",
        "print(f\"Std Confidence: {np.std(confidence_scores):.4f}\")\n",
        "print(f\"Min Confidence: {np.min(confidence_scores):.4f}\")\n",
        "print(f\"Max Confidence: {np.max(confidence_scores):.4f}\")\n",
        "\n",
        "\n",
        "low_confidence_threshold = 0.7\n",
        "low_confidence_indices = np.where(confidence_scores < low_confidence_threshold)[0]\n",
        "print(f\"Low confidence predictions (< {low_confidence_threshold}): {len(low_confidence_indices)} / {len(confidence_scores)} ({len(low_confidence_indices)/len(confidence_scores)*100:.1f}%)\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(confidence_scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(np.mean(confidence_scores), color='red', linestyle='--',\n",
        "           label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
        "plt.axvline(low_confidence_threshold, color='orange', linestyle='--',\n",
        "           label=f'Threshold: {low_confidence_threshold}')\n",
        "plt.title('Prediction Confidence Distribution - Ensemble Model', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "ensemble_model.save(\"ensemble_garbage_classifier.keras\")\n",
        "print(\" .keras format saved\")\n",
        "\n",
        "print(\"Converting to TensorFlow Lite...\")\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(ensemble_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.representative_dataset = None\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(config.TFLITE_MODEL, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\" TFLite model saved: {config.TFLITE_MODEL}\")\n",
        "\n",
        "import os\n",
        "h5_size = os.path.getsize(config.ENSEMBLE_MODEL) / (1024*1024)\n",
        "keras_size = os.path.getsize(\"ensemble_garbage_classifier.keras\") / (1024*1024)\n",
        "tflite_size = os.path.getsize(config.TFLITE_MODEL) / (1024*1024)\n",
        "\n",
        "print(f\" H5 Format: {h5_size:.1f} MB\")\n",
        "print(f\"Keras Format: {keras_size:.1f} MB\")\n",
        "print(f\"TFLite Format: {tflite_size:.1f} MB\")\n",
        "print(f\" Compression Ratio: {h5_size/tflite_size:.1f}x smaller\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def predict_garbage(image):\n",
        "    try:\n",
        "\n",
        "        if image is None:\n",
        "            return \" Lütfen bir görsel yükleyin!\"\n",
        "        img = image.resize(config.IMG_SIZE)\n",
        "        img_array = np.array(img) / 255.0\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        predictions = ensemble_model.predict(img_array, verbose=0)\n",
        "\n",
        "        top_indices = np.argsort(predictions[0])[::-1][:3]\n",
        "\n",
        "        results = {}\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            confidence = predictions[0][idx]\n",
        "            class_name = class_names[idx]\n",
        "            results[f\"{i+1}. {class_name}\"] = f\"{confidence:.3f} ({confidence*100:.1f}%)\"\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\" Hata: {str(e)}\"\n",
        "\n",
        "def predict_with_visualization(image):\n",
        "    \"\"\"Görselleştirme ile prediction\"\"\"\n",
        "    try:\n",
        "        if image is None:\n",
        "            return \"Lütfen bir görsel yükleyin!\", None\n",
        "        img = image.resize(config.IMG_SIZE)\n",
        "        img_array = np.array(img) / 255.0\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        predictions = ensemble_model.predict(img_array, verbose=0)[0]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        bars = ax.bar(class_names, predictions, color='skyblue', alpha=0.7)\n",
        "\n",
        "        max_idx = np.argmax(predictions)\n",
        "        bars[max_idx].set_color('red')\n",
        "        bars[max_idx].set_alpha(0.8)\n",
        "\n",
        "        ax.set_title(f'Ensemble Model Predictions\\nTop: {class_names[max_idx]} ({predictions[max_idx]*100:.1f}%)',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Confidence Score')\n",
        "        ax.set_xlabel('Waste Categories')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        plot_img = Image.open(buf)\n",
        "        plt.close()\n",
        "\n",
        "        top_pred = class_names[max_idx]\n",
        "        confidence = predictions[max_idx]\n",
        "\n",
        "        result_text = f\"\"\"\n",
        "         **ENSEMBLE MODEL PREDICTION**\n",
        "\n",
        "        **Top Prediction:** {top_pred}\n",
        "        **Confidence:** {confidence:.3f} ({confidence*100:.1f}%)\n",
        "\n",
        "        **All Predictions:**\n",
        "        \"\"\"\n",
        "\n",
        "        for i, (name, conf) in enumerate(zip(class_names, predictions)):\n",
        "            result_text += f\"\\n{i+1}. {name}: {conf:.3f} ({conf*100:.1f}%)\"\n",
        "\n",
        "        return result_text, plot_img\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\" Hata: {str(e)}\", None\n",
        "simple_interface = gr.Interface(\n",
        "    fn=predict_garbage,\n",
        "    inputs=gr.Image(type=\"pil\", label=\" Çöp Fotoğrafı Yükleyin\"),\n",
        "    outputs=gr.Label(num_top_classes=3, label=\" Tahmin Sonuçları\"),\n",
        "    title=\"AI Ensemble Çöp Sınıflandırıcı\",\n",
        "    description=\"\"\"\n",
        "    **3-Model Ensemble (EfficientNetB3 + ResNet50V2 + DenseNet121)**\n",
        "\n",
        "    Çöp fotoğrafınızı yükleyin ve AI'ın hangi kategoriye ait olduğunu tahmin etmesini izleyin!\n",
        "\n",
        "    **Kategoriler:** Cardboard, Glass, Metal, Paper, Plastic, Trash\n",
        "    \"\"\",\n",
        "    examples=None,\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "advanced_interface = gr.Interface(\n",
        "    fn=predict_with_visualization,\n",
        "    inputs=gr.Image(type=\"pil\", label=\" Çöp Fotoğrafı\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Detaylı Sonuç\"),\n",
        "        gr.Image(label=\" Confidence Chart\")\n",
        "    ],\n",
        "    title=\"Advanced Ensemble Çöp Sınıflandırıcı\",\n",
        "    description=\"\"\"\n",
        "    **Gelişmiş Analiz:** Tüm sınıflar için confidence skorları ve görselleştirme\n",
        "    \"\"\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "demo = gr.TabbedInterface(\n",
        "    [simple_interface, advanced_interface],\n",
        "    [\"Basit Tahmin\", \" Detaylı Analiz\"],\n",
        "    title=\" Ensemble AI Çöp Sınıflandırıcı\"\n",
        ")\n",
        "\n",
        "print(\"=\" * 40)\n",
        "\n",
        "demo.launch(\n",
        "    share=True,\n",
        "    debug=True,\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Model Performance Summary:\")\n",
        "print(f\"    Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "print(f\"    F1-Score: {macro_f1:.3f}\")\n",
        "print(f\"    Best Class: {class_names[best_idx]} ({f1[best_idx]:.3f})\")\n",
        "print(f\"    Worst Class: {class_names[worst_idx]} ({f1[worst_idx]:.3f})\")\n",
        "print(f\"    {config.ENSEMBLE_MODEL} ({h5_size:.1f} MB)\")\n",
        "print(f\"    ensemble_garbage_classifier.keras ({keras_size:.1f} MB)\")\n",
        "print(f\"    {config.TFLITE_MODEL} ({tflite_size:.1f} MB)\")\n"
      ],
      "metadata": {
        "id": "H--O3Ai5W053"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}